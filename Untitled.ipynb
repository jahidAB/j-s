{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb2f04d",
   "metadata": {},
   "source": [
    "# cie4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82cbd4",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/tutorial-machine-learning-pipelines-mlops-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc94fd6",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/reduce-data-dimentionality-using-pca-python/?ref=rp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968f578",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d4b04",
   "metadata": {},
   "source": [
    "### Pipeline(defination,working, diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63bb19",
   "metadata": {},
   "source": [
    "The term Pipeline is used generally to describe the independent sequence of steps that are arranged together to achieve a task. This task could be machine learning or not.\n",
    "\n",
    "The deployment of machine learning models (or pipelines) is the process of making models available in production where web applications, enterprise software (ERPs) and APIs can consume the trained model by providing new data points, and get the predictions\n",
    "\n",
    "A machine learning pipeline is a way to control and automate the workflow it takes to produce a machine learning model. Machine learning pipelines consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment.\n",
    "\n",
    "Machine learning pipelines are iterative as every step is repeated to continuously improve the accuracy of the model and achieve the end goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b759643",
   "metadata": {},
   "source": [
    "https://res.cloudinary.com/dyd911kmh/image/upload/v1647003191/image5_f3ko0i.png\n",
    "    \n",
    "    \n",
    "    reffer the above image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd566fb",
   "metadata": {},
   "source": [
    "### Defination & Steps of pca with code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70a4ba",
   "metadata": {},
   "source": [
    "Principal Component Analysis is a technique of feature extraction that maps a higher dimensional feature space to a lower-dimensional feature space. While reducing the number of dimensions, PCA ensures that maximum information of the original dataset is retained in the dataset with the reduced no. of dimensions and the co-relation between the newly obtained Principal Components is minimum. T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac43eb9",
   "metadata": {},
   "source": [
    "**Steps  to  Apply  PCA  in  Python  for  Dimensionality  Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145b996",
   "metadata": {},
   "source": [
    "**Step-1:**\n",
    "\n",
    "Import necessary libraries\n",
    "\n",
    "All the necessary libraries required to load the dataset, pre-process it and then apply PCA on it are mentioned below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c741f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets  # to retrieve the iris Dataset\n",
    "# import pandas as pd  # to load the dataframe\n",
    "# from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "# from sklearn.decomposition import PCA  # to apply PCA\n",
    "# import seaborn as sns  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7707615",
   "metadata": {},
   "source": [
    "**Step-2:** \n",
    "\n",
    "Load the dataset\n",
    "\n",
    "After importing all the necessary libraries, we need to load the dataset. Now, the iris dataset is already present in sklearn. First, we will load it and then convert it into a pandas data frame for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1aaa2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris = datasets.load_iris()\n",
    "# #convert the dataset into a pandas data frame\n",
    "# df = pd.DataFrame(iris['data'], columns = iris['feature_names'])\n",
    "# #display the head (first 5 rows) of the dataset\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c2731",
   "metadata": {},
   "source": [
    "**Step-3:**\n",
    "\n",
    "Standardize the features\n",
    "\n",
    "Before applying PCA or any other Machine Learning technique it is always considered good practice to standardize the data. For this, Standard Scalar is the most commonly used scalar. Standard Scalar is already present in sklearn. So, now we will standardize the feature set using Standard Scalar and store the scaled feature set as a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27442d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar = StandardScaler()\n",
    "# scaled_data = pd.DataFrame(scalar.fit_transform(df)) #scaling the data\n",
    "# scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bfb09",
   "metadata": {},
   "source": [
    "**Step-4:**\n",
    "\n",
    "Check the Co-relation between features without PCA (Optional)\n",
    "\n",
    " \n",
    "\n",
    "Now, we will check the co-relation between our scaled dataset using a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a297221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(scaled_data.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c55d38",
   "metadata": {},
   "source": [
    "We can observe from the above heatmap that sepal length & petal length and petal length & petal width have high co-relation. Thus, we evidently need to apply dimensionality reduction. If you are already aware that your dataset needs dimensionality reduction – you can skip this step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55fdef",
   "metadata": {},
   "source": [
    "**Step-5:** \n",
    "\n",
    "Applying Principal Component Analysis\n",
    "\n",
    "We need to create an object of PCA  and while doing so we also need to initialize n_components – which is the number of principal components we want in our final dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc84332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 3)\n",
    "# pca.fit(scaled_data)\n",
    "# data_pca = pca.transform(scaled_data)\n",
    "# data_pca = pd.DataFrame(data_pca,columns=['PC1','PC2','PC3'])\n",
    "# data_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8695e",
   "metadata": {},
   "source": [
    "**Step-6:**\n",
    "\n",
    "Checking Co-relation between features after PCA\n",
    "\n",
    "Now that we have applied PCA and obtained the reduced feature set, we will check the co-relation between various Principal Components, again by using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4867e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(data_pca.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ecca9",
   "metadata": {},
   "source": [
    "The above heatmap clearly depicts that there is no correlation between various obtained principal components (PC1, PC2, and PC3). Thus, we have moved from higher dimensional feature space to a lower-dimensional feature space while ensuring that there is no correlation between the so obtained PCs is minimum. Hence, we have accomplished the objectives of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b3f45",
   "metadata": {},
   "source": [
    "### Ensemble technique (max voting, averaging,weighted average,bagging, boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004b017",
   "metadata": {},
   "source": [
    " **MAX VOTING**\n",
    " \n",
    "The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "For example, when you asked 5 of your colleagues to rate your movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n",
    "The result of max voting would be something like this:\n",
    "\n",
    "Colleague 1  --  5\n",
    "\n",
    "Colleague 2\t  --  4\n",
    "\n",
    "Colleague 3\t  --  5\n",
    "\n",
    "Colleague 4\t  --  4\n",
    "\n",
    "Colleague 5  --  4\n",
    "\n",
    "#Final rating   --  4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579803d4",
   "metadata": {},
   "source": [
    " **AVERAGING**\n",
    " \n",
    "Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
    "\n",
    "For example, in the below case, the averaging method would take the average of all the values.\n",
    "\n",
    "i.e. (5+4+5+4+4)/5 = 4.4\n",
    "\n",
    "Colleague 1   -- 5\n",
    "\n",
    "Colleague 2\t  -- 4\n",
    "\n",
    "Colleague 3\t  -- 5\n",
    "\n",
    "Colleague 4\t  -- 4\n",
    "\n",
    "Colleague 5\t  -- 4\n",
    "\n",
    "#Final rating   -- 4.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577dbc8",
   "metadata": {},
   "source": [
    "**WEIGHTED AVERAGE**\n",
    "\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n",
    "\n",
    "The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41.\n",
    "\n",
    "Colleague 1 -- 5  --  w(0.23)\n",
    "\n",
    "Colleague 2 -- 4  --  w(0.23)\n",
    "\n",
    "Colleague 3 -- 5  --  w(0.18)\n",
    "\n",
    "Colleague 4 -- 4  --  w(0.18)\n",
    "\n",
    "Colleague 5 -- 4  --  w(0.18)\n",
    "\n",
    "#Final rating  --  4.41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e75858",
   "metadata": {},
   "source": [
    "**BAGGING**\n",
    "\n",
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result\n",
    "\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n",
    "\n",
    "Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n",
    "\n",
    "*. Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "\n",
    "*. A base model (weak model) is created on each of these subsets.\n",
    "\n",
    "*. The models run in parallel and are independent of each other.\n",
    "\n",
    "*. The final predictions are determined by combining the predictions from all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624b182",
   "metadata": {},
   "source": [
    "**BOOSTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9247c",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let’s understand the way boosting works in the below steps.\n",
    "\n",
    "*.A subset is created from the original dataset.\n",
    "\n",
    "*.Initially, all data points are given equal weights.\n",
    "\n",
    "*.A base model is created on this subset.\n",
    "\n",
    "*.This model is used to make predictions on the whole dataset\n",
    "\n",
    "*.Errors are calculated using the actual values and predicted values.\n",
    "\n",
    "*.The observations which are incorrectly predicted, are given higher weights.\n",
    "(Here, the three misclassified blue-plus points will be given higher weights)\n",
    "\n",
    "*.Another model is created and predictions are made on the dataset.\n",
    "(This model tries to correct the errors from the previous model)\n",
    "\n",
    "*.Similarly, multiple models are created, each correcting the errors of the previous model.\n",
    "\n",
    "*.The final model (strong learner) is the weighted mean of all the models (weak learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4bbaf",
   "metadata": {},
   "source": [
    "**BAGGING ALGORITHM**\n",
    "\n",
    "Bagging meta-estimator\n",
    "\n",
    "Random forest\n",
    "\n",
    "\n",
    "\n",
    "**BOOSTING ALGORITHM:**\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "GBM\n",
    "\n",
    "XGBM\n",
    "\n",
    "Light GBM\n",
    "\n",
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b4dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10541f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
